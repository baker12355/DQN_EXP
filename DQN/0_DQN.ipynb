{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Deep Q Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, random, pickle, os.path, math, glob\n",
    "import numpy as np\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.wrappers import *\n",
    "from utils.hyperparameters import Config\n",
    "from utils.plot import plot_all_data\n",
    "from utils.plot import *\n",
    "\n",
    "from agents.BaseAgent import BaseAgent\n",
    "from functions import *\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_profit(log_dir, step, profit):\n",
    "    with open(os.path.join(log_dir, 'profit.csv'), 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow((step, profit))\n",
    "        \n",
    "def plot_sigma(profit_TJ, log_dir, frame_idx, title):\n",
    "    l = profit_TJ[0].shape[0]\n",
    "    X = np.reshape(profit_TJ, (-1, l))[-15:]\n",
    "    X = X.cumsum(axis = 1)\n",
    "    t = np.arange(X.shape[1])\n",
    "\n",
    "    mu1 = X.mean(axis=0)\n",
    "    sigma1 = X.std(axis=0)\n",
    "\n",
    "    # plot it!\n",
    "    fig, ax = plt.subplots(1, figsize=(20, 5),)\n",
    "    ax.plot(t, mu1, lw=2, label='mean', color='blue', alpha=0.5)\n",
    "    ax.plot(t, X[-1], lw=2, label='now', color='red')\n",
    "    ax.plot(t, [0 for i in range(len(t))], color='black', alpha=0.5)\n",
    "    ax.fill_between(t, mu1+sigma1, mu1-sigma1, facecolor='blue', alpha=0.3)\n",
    "    ax.fill_between(t, mu1+sigma1*2, mu1-sigma1*2, facecolor='blue', alpha=0.1)\n",
    "    ax.set_title('Mean & Std')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_xlabel('num steps')\n",
    "    ax.set_ylabel(title)\n",
    "    fig = plt.gcf()\n",
    "    plt.show()\n",
    "    fig.savefig(log_dir + '/' + title + '_log/' + title+'_'+str(frame_idx)+'.png')\n",
    "    plt.close()\n",
    "\n",
    "def run_testing(model, env):\n",
    "    env_id = 'Trading_system'\n",
    "    episode_reward = 0\n",
    "    observation = env.reset()\n",
    "    while(1):\n",
    "        action = model.get_action(observation, 0)\n",
    "        prev_observation=observation\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        observation = None if done else observation\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            print ('Testing: profit: ', env.profit_df.sum())\n",
    "            model.finish_nstep()\n",
    "            model.reset_hx()\n",
    "            break\n",
    "    Total_profit = env.profit_df.sum()\n",
    "    env.writeaction(log_dir)\n",
    "    return Total_profit, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class action_space():\n",
    "    def __init__(self, a):\n",
    "        self.n = a\n",
    "\n",
    "class environment():\n",
    "    def __init__(self, df, log_dir, firstday, lastday, transection=0.001, delay=10, training=1):\n",
    "        self.delay = delay\n",
    "        self.n_features = df.shape[1]\n",
    "        \n",
    "        self.firstday_orig = firstday\n",
    "        self.lastday_orig = lastday\n",
    "        self.training = training\n",
    "        \n",
    "        #self.firstday = firstday\n",
    "        #self.lastday = lastday\n",
    "        \n",
    "        self.action_space = action_space(3)\n",
    "        self.observation_space = Box(high=5000, low=-5000, shape=(1, self.n_features*delay))\n",
    "        self.df_orig = df.copy().reset_index(drop=True)\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "        self.env_id = None\n",
    "        self.today = None\n",
    "        self.position = None\n",
    "        self.holding_prize = None\n",
    "        self.profit_df = np.array([])\n",
    "        self.reward_df = np.array([])\n",
    "        self.action_df = np.array([])\n",
    "        self.transection = transection\n",
    "        self.counter = 0\n",
    "        self.reward_list = []\n",
    "        self.profit_list = []\n",
    "        self.action_list = []\n",
    "        self.buffer = []\n",
    "        self.init()\n",
    "\n",
    "    def init(self):\n",
    "        cols = [col for col in self.df_orig.columns if col not in ['sell', 'hold', 'buy', 'profit', 'allocation']]\n",
    "        self.head = self.df_orig.iloc[self.firstday_orig]\n",
    "        self.min_ = self.df_orig[cols].min()\n",
    "        self.max_ = self.df_orig[cols].max()\n",
    "        self.range_ = self.max_-self.min_\n",
    "        for index in self.range_.index:\n",
    "            self.df_orig.loc[:, index] = (self.df_orig[index] - self.min_[index]).values\n",
    "            self.df_orig.loc[:, index] = (self.df_orig[index] / self.range_[index]).values\n",
    "\n",
    "    def get_shapre(self, profit):\n",
    "        self.buffer.append(profit)\n",
    "        if len(self.buffer)<=1:\n",
    "            return 0\n",
    "        if len(self.buffer)>5:\n",
    "            self.buffer = self.buffer[1:]\n",
    "        mean = np.mean(self.buffer)\n",
    "        std = np.std(self.buffer)\n",
    "        if std ==0:\n",
    "            return 0\n",
    "        return mean/std\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.counter += 1\n",
    "        if self.today == self.lastday-1:\n",
    "            done = 1  \n",
    "            observation = None\n",
    "            self.reward_list.append(self.reward_df)\n",
    "            self.profit_list.append(self.profit_df)\n",
    "            self.action_list.append(self.action_df)\n",
    "            return None, 0, done, _\n",
    "        else :\n",
    "            done = 0\n",
    "        if self.today == self.lastday-2:\n",
    "            action = 1\n",
    "        old_state_n = self.position\n",
    "        new_state_n = action\n",
    "        old_holding = self.holding_prize\n",
    "        new_holding = self.df.loc[self.today+1, 'open']\n",
    "        \n",
    "        #print ('action: ', action)\n",
    "        # Update transection cost\n",
    "        #if old_state_n == new_state_n:\n",
    "        #    transection_fee = 0\n",
    "        #elif abs(old_state_n - new_state_n)==2:\n",
    "        #    transection_fee = new_holding*2*self.transection\n",
    "        #else:\n",
    "        #    transection_fee = new_holding*1*self.transection\n",
    "        transection_fee = 0\n",
    "        if (old_state_n!=1) & (old_state_n-new_state_n)!=0:\n",
    "            transection_fee = self.holding_prize*self.transection\n",
    "        #print ('old_state_n: ', old_state_n)\n",
    "        #print ('new_state_n: ', new_state_n)\n",
    "        #print ('transection_fee: ', transection_fee)\n",
    "        #print ('----------------------------------------------')\n",
    "        #-----------------------------------------------\n",
    "        \n",
    "        # Update profit\n",
    "        if (old_state_n==1) | (old_state_n==new_state_n):\n",
    "            profit = 0 - transection_fee\n",
    "        else:\n",
    "            profit = (new_holding - old_holding)*(old_state_n-1) - transection_fee\n",
    "        profit = profit * self.range_.open / self.head.open\n",
    "        #print ('old_holding', old_holding)\n",
    "        #print ('new_holding', new_holding)\n",
    "        #print ('profit: ', profit)\n",
    "        #print ('old_state_n: ', old_state_n)\n",
    "        #print ('new_state_n: ', new_state_n)\n",
    "        #print ('old_holding: ', old_holding)\n",
    "        #print ('new_holding: ', new_holding)\n",
    "        #print ('profit: ', profit)\n",
    "        #print ('----------------------------------------------')\n",
    "        \n",
    "        # Update holding prize\n",
    "        if new_state_n == 1:\n",
    "            self.holding_prize = 0\n",
    "        elif old_state_n != new_state_n:\n",
    "            self.holding_prize = new_holding\n",
    "        #print ('holding_prize: ', self.holding_prize)\n",
    "        #print ('old_state_n: ', old_state_n)\n",
    "        #print ('new_state_n: ', new_state_n)\n",
    "        #print ('holding_prize: ', new_holding)\n",
    "        #print ('----------------------------------------------')\n",
    "        # ------------------------------------\n",
    "        \n",
    "        old_prize = self.df.loc[self.today, 'open']\n",
    "        new_prize = self.df.loc[self.today+1, 'open']\n",
    "        \n",
    "        # Update reward\n",
    "        if new_state_n==1:\n",
    "            reward = 0\n",
    "        elif new_state_n==0:\n",
    "            reward = np.log(old_prize / (new_prize+transection_fee))\n",
    "        else:\n",
    "            reward = np.log((new_prize-transection_fee)/old_prize)\n",
    "        #print ('old_state_n: ', old_state_n)\n",
    "        #print ('new_state_n: ', new_state_n)\n",
    "        #print ('old_prize: ', old_prize)\n",
    "        #print ('new_prize: ', new_prize)\n",
    "        #print ('reward: ', reward)\n",
    "        #print ('----------------------------------------------')\n",
    "        #reward = self.get_shapre(reward)\n",
    "        reward *= 1\n",
    "        self.log_PRA(profit, reward, action)\n",
    "        self.set_PRA(profit, self.holding_prize, action)\n",
    "        self.position = action\n",
    "        observation = self.get_state_seq(self.today+1)\n",
    "        #print ('old_prize', old_prize)\n",
    "        #print ('new_prize', new_prize)\n",
    "        #print ('reward: ', reward)        \n",
    "        self.today+=1\n",
    "        return observation, reward, done, _ \n",
    "\n",
    "    def get_state_seq(self, step):\n",
    "        #print ('step: ', step)\n",
    "        step += 1\n",
    "        if (step < self.delay) & (self.training):\n",
    "            head = self.delay-step\n",
    "            header = self.df_orig[self.firstday_orig-head:self.firstday_orig] \n",
    "            tail = self.delay-head\n",
    "            tailer = self.df[:tail] \n",
    "            state = header.append(tailer).values.flatten()\n",
    "            #print ('head', head)\n",
    "        else:\n",
    "            state = self.df.iloc[step-self.delay:step].values.flatten()\n",
    "        state = np.expand_dims(state, 0)\n",
    "        return state\n",
    "\n",
    "    def set_PRA(self, profit, allocation, action):\n",
    "        self.df.loc[self.today+1, 'profit'] = profit\n",
    "        self.df.loc[self.today+1, 'allocation'] = self.holding_prize\n",
    "        arr = [0,0,0]\n",
    "        arr[action] = 1\n",
    "        self.df.loc[self.today+1, ['sell', 'hold', 'buy']] = arr\n",
    "        \n",
    "    def log_PRA(self, profit, reward, action):\n",
    "        self.profit_df = np.append(self.profit_df, profit)\n",
    "        self.reward_df = np.append(self.reward_df, reward)\n",
    "        self.action_df = np.append(self.action_df, action)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.sub_df()\n",
    "        self.today = self.firstday\n",
    "        self.position = 1\n",
    "        self.holding_prize = 0\n",
    "        self.buffer = []\n",
    "        \n",
    "        self.profit_df = np.array([])\n",
    "        self.reward_df = np.array([])\n",
    "        self.action_df = np.array([])\n",
    "        s = self.get_state_seq(self.today)\n",
    "        return s\n",
    "    def close(self):\n",
    "        pass\n",
    "    def writeaction(self, log_dir):\n",
    "        with open(os.path.join('%s/%s.csv'%(log_dir, 'action')), 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(self.action_list[-1])\n",
    "\n",
    "    def sub_df(self):\n",
    "        if self.training:\n",
    "            sub_inx = np.random.choice(range(self.firstday_orig+1, self.lastday_orig), int((self.lastday_orig-self.firstday_orig)/4) )\n",
    "            sub_inx.sort()\n",
    "            self.df = self.df_orig.iloc[sub_inx].copy().reset_index(drop=True)\n",
    "            self.firstday = 0\n",
    "            self.lastday = self.df.shape[0]\n",
    "        else:\n",
    "            self.df = self.df_orig\n",
    "            self.firstday = self.firstday_orig\n",
    "            self.lastday = self.lastday_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#epsilon variables\n",
    "config.epsilon_start    = 1.0\n",
    "config.epsilon_final    = 0.1\n",
    "config.epsilon_decay    = 30000\n",
    "config.epsilon_by_frame = lambda frame_idx: config.epsilon_final + (config.epsilon_start - config.epsilon_final) * math.exp(-1. * frame_idx / config.epsilon_decay)\n",
    "\n",
    "#misc agent variables\n",
    "config.GAMMA = 0.99\n",
    "config.LR    = 1e-4\n",
    "\n",
    "#memory\n",
    "config.TARGET_NET_UPDATE_FREQ = 1000\n",
    "config.EXP_REPLAY_SIZE        = 25000\n",
    "config.BATCH_SIZE             = 32\n",
    "\n",
    "#Learning control variables\n",
    "config.LEARN_START = 10000\n",
    "config.MAX_FRAMES  = 250000\n",
    "config.UPDATE_FREQ = 1\n",
    "\n",
    "#data logging parameters\n",
    "config.ACTION_SELECTION_COUNT_FREQUENCY = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size), None, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Network Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape[1]\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_shape, 256)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   \n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc2.weight.data.normal_(0, 0.1)  \n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc3.weight.data.normal_(0, 0.1)  \n",
    "        self.out = nn.Linear(256, num_actions)\n",
    "        self.out.weight.data.normal_(0, 0.1)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(BaseAgent):\n",
    "    def __init__(self, static_policy=False, env=None, config=None, log_dir='/tmp/gym'):\n",
    "        super(Model, self).__init__(config=config, env=env, log_dir=log_dir)\n",
    "        self.device = config.device\n",
    "\n",
    "        self.gamma = config.GAMMA\n",
    "        self.lr = config.LR\n",
    "        self.target_net_update_freq = config.TARGET_NET_UPDATE_FREQ\n",
    "        self.experience_replay_size = config.EXP_REPLAY_SIZE\n",
    "        self.batch_size = config.BATCH_SIZE\n",
    "        self.learn_start = config.LEARN_START\n",
    "        self.update_freq = config.UPDATE_FREQ\n",
    "\n",
    "        self.static_policy = static_policy\n",
    "        self.num_feats = env.observation_space.shape\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.env = env\n",
    "\n",
    "        self.declare_networks()\n",
    "            \n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-8)\n",
    "        \n",
    "        #move to correct device\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.target_model.to(self.device)\n",
    "\n",
    "        if self.static_policy:\n",
    "            self.model.eval()\n",
    "            self.target_model.eval()\n",
    "        else:\n",
    "            self.model.train()\n",
    "            self.target_model.train()\n",
    "\n",
    "        self.update_count = 0\n",
    "\n",
    "        self.declare_memory()\n",
    "\n",
    "    def declare_networks(self):\n",
    "        self.model = DQN(self.num_feats, self.num_actions)\n",
    "        self.target_model = DQN(self.num_feats, self.num_actions)\n",
    "\n",
    "    def declare_memory(self):\n",
    "        self.memory = ExperienceReplayMemory(self.experience_replay_size)\n",
    "\n",
    "    def append_to_replay(self, s, a, r, s_):\n",
    "        self.memory.push((s, a, r, s_))\n",
    "\n",
    "    def prep_minibatch(self):\n",
    "        # random transition batch is taken from experience replay memory\n",
    "        transitions, indices, weights = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        batch_state, batch_action, batch_reward, batch_next_state = zip(*transitions)\n",
    "\n",
    "        shape = (-1,)+self.num_feats\n",
    "\n",
    "        batch_state = torch.tensor(batch_state, device=self.device, dtype=torch.float).view(shape)\n",
    "        batch_action = torch.tensor(batch_action, device=self.device, dtype=torch.long).squeeze().view(-1, 1)\n",
    "        batch_reward = torch.tensor(batch_reward, device=self.device, dtype=torch.float).squeeze().view(-1, 1)\n",
    "        \n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch_next_state)), device=self.device, dtype=torch.uint8)\n",
    "        try: #sometimes all next states are false\n",
    "            non_final_next_states = torch.tensor([s for s in batch_next_state if s is not None], device=self.device, dtype=torch.float).view(shape)\n",
    "            empty_next_state_values = False\n",
    "        except:\n",
    "            non_final_next_states = None\n",
    "            empty_next_state_values = True\n",
    "\n",
    "        return batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights\n",
    "\n",
    "    def compute_loss(self, batch_vars):\n",
    "        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights = batch_vars\n",
    "\n",
    "        #estimate\n",
    "        current_q_values = self.model(batch_state).squeeze().gather(1, batch_action)\n",
    "\n",
    "        #target\n",
    "        with torch.no_grad():\n",
    "            max_next_q_values = torch.zeros(self.batch_size, device=self.device, dtype=torch.float).unsqueeze(dim=1)\n",
    "            if not empty_next_state_values:\n",
    "                max_next_action = self.get_max_next_state_action(non_final_next_states)\n",
    "                max_next_q_values[non_final_mask] = self.target_model(non_final_next_states).squeeze().gather(1, max_next_action)\n",
    "            expected_q_values = batch_reward + self.gamma*max_next_q_values\n",
    "\n",
    "        diff = (expected_q_values - current_q_values)\n",
    "        loss = self.MSE(diff)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def update(self, s, a, r, s_, frame=0):\n",
    "        if self.static_policy:\n",
    "            return None\n",
    "\n",
    "        self.append_to_replay(s, a, r, s_)\n",
    "\n",
    "        if frame < self.learn_start or frame % self.update_freq != 0:\n",
    "            return None\n",
    "\n",
    "        batch_vars = self.prep_minibatch()\n",
    "\n",
    "        loss = self.compute_loss(batch_vars)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.update_target_model()\n",
    "        self.save_td(loss.item(), frame)\n",
    "        self.save_sigma_param_magnitudes(frame)\n",
    "\n",
    "    def get_action(self, s, eps=0.1):\n",
    "        with torch.no_grad():\n",
    "            if np.random.random() >= eps or self.static_policy:\n",
    "                X = torch.tensor([s], device=self.device, dtype=torch.float)\n",
    "                a = self.model(X).max(2)[1].view(1, 1)\n",
    "                return a.item()\n",
    "            else:\n",
    "                return np.random.randint(0, self.num_actions)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.update_count+=1\n",
    "        self.update_count = self.update_count % self.target_net_update_freq\n",
    "        if self.update_count == 0:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def get_max_next_state_action(self, next_states):\n",
    "        return self.target_model(next_states).max(dim=2)[1].view(-1, 1)\n",
    "\n",
    "    def finish_nstep(self):\n",
    "        pass\n",
    "    \n",
    "    def reset_hx(self):\n",
    "        pass\n",
    "    \n",
    "    def MSE(self, x):\n",
    "        return 0.5 * x.pow(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functions import *\n",
    "data_0 = load_data('../NAS10_mkt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = ['open', 'high', 'low', 'close', 'volume', 'sell', 'hold', 'buy', 'profit', 'allocation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系統找不到指定的路徑。: 'exp_/0_DQN/Tesla, Inc. /exp_plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-29024fec9754>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0massets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massetName\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msub_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'exp_plot'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'saved_agents'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'profit_log'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reward_log'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_result_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0massets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\DQN_EXP\\DQN\\functions.py\u001b[0m in \u001b[0;36mmkdir\u001b[1;34m(exp_result_path, project_name, assets, sub_folder)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msub_folder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s/%s/%s/%s'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_result_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0masset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s/%s/%s/%s'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_result_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0masset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_folder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系統找不到指定的路徑。: 'exp_/0_DQN/Tesla, Inc. /exp_plot'"
     ]
    }
   ],
   "source": [
    "exp_result_path = 'exp_'\n",
    "project_name = '0_DQN'\n",
    "assets = data_0.assetName.unique()\n",
    "sub_folder = ['exp_plot', 'saved_agents', 'profit_log', 'reward_log']\n",
    "mkdir(exp_result_path, project_name, assets, sub_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_0[data_0.assetName=='Apple Inc.']\n",
    "train_firstday = df[df.date>=('2014-01-01')].index[0]\n",
    "train_lastday = df[df.date<=('2017-12-31')].index[-1]\n",
    "\n",
    "test_firstday = df[df.date>=('2018-01-01')].index[0]\n",
    "test_lastday = df[df.date<=('2018-12-31')].index[-1]\n",
    "cols =['open', 'high', 'low', 'close', 'volume', 'month', 'day', 'dayofweek', 'sell',\n",
    "       'hold', 'buy', 'profit', 'allocation', 'SMA', 'macd', 'macdsignal',\n",
    "       'macdhist', 'RSI', 'WILLR', 'STOCH_D', 'STOCH_K', 'ROCP']\n",
    "delay = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for asset in assets:\n",
    "    start=timer()\n",
    "    \n",
    "    log_dir = '%s/%s/%s'%(exp_result_path, project_name, asset)\n",
    "\n",
    "    data = data_0[data_0.assetName==asset]\n",
    "    data = TI(data)\n",
    "    #data = nomorlize(data)\n",
    "    env_id = '0_DQN'\n",
    "    env = environment(data[cols], log_dir, train_firstday, train_lastday, delay=delay, training=False)\n",
    "    env2 = environment(data[cols], '', test_firstday, test_lastday, delay=delay, training=False)\n",
    "    \n",
    "    env.env_id = env_id\n",
    "    model = Model(env=env, config=config, log_dir=log_dir)\n",
    "\n",
    "    episode_reward = 0\n",
    "    observation = env.reset()\n",
    "    for frame_idx in range(1, config.MAX_FRAMES + 1):\n",
    "        epsilon = config.epsilon_by_frame(frame_idx)\n",
    "\n",
    "        action = model.get_action(observation, epsilon)\n",
    "        model.save_action(action, frame_idx) #log action selection\n",
    "\n",
    "        prev_observation=observation\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        observation = None if done else observation\n",
    "        #print (reward)\n",
    "        model.update(prev_observation, action, reward, observation, frame_idx)\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            model.finish_nstep()\n",
    "            model.reset_hx()\n",
    "            observation = env.reset()\n",
    "            model.save_reward(env_id, frame_idx, episode_reward)\n",
    "            episode_reward = 0\n",
    "            \n",
    "        if (frame_idx % 10000 == 0):\n",
    "            model.save_w()\n",
    "            try:\n",
    "                clear_output(True)\n",
    "                plot_all_data(log_dir, env_id, '0_DQN', config.MAX_FRAMES, bin_size=(10, 100, 100, 1), smooth=1, time=timedelta(seconds=int(timer()-start)), ipynb=True)\n",
    "            except IOError:\n",
    "                pass\n",
    "\n",
    "        if (frame_idx % 1000 == 0) & (frame_idx>config.MAX_FRAMES/8):\n",
    "            Total_profit, env2 = run_testing(model, env2)\n",
    "            save_profit(log_dir, frame_idx, env2.profit_df.sum())\n",
    "            if frame_idx % 10000 == 0:\n",
    "                plot_sigma(env2.reward_list, log_dir, frame_idx, 'reward')\n",
    "                plot_sigma(env2.profit_list, log_dir, frame_idx, 'profit')\n",
    "plot_all_data(log_dir, env_id, '0_DQN', config.MAX_FRAMES, bin_size=(10, 100, 100, 1), smooth=1, time=timedelta(seconds=int(timer()-start)), ipynb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
